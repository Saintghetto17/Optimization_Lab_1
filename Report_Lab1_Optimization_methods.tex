\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{dsfont}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{imakeidx}
\usepackage[russian]{cleveref}
\usepackage{array}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\newcolumntype{C}{>{$}c<{$}}
\usepackage[a4paper,left=15mm,right=15mm,top=15mm,bottom=20mm]{geometry}
\parindent=0mm
\parskip=3mm

\makeindex
\pagestyle{empty}

\title{Методы оптимизации.\\ Отчет по лабораторной работе 1. \\Новицкий Илья, Остриченко Илья, Петров Георгий. М3234}

\date{19.03.2024}


\begin{document}

\maketitle

Ссылка на GitHub : \url{https://github.com/Saintghetto17/Optimization_Lab_1}

\large{Постановка задачи:} найти минимальное значение функции $f : \mathbb{R}^n  \rightarrow \mathbb{R}$, используя  методы 0-1 порядков. \\
В нашем случае пришлось работать с квадратичными функциями при $n = 2$. В качестве исследуемых функций мы взяли:
\begingroup
\renewcommand\labelenumi{(\theenumi)}
\begin{enumerate}
\item  $f_{1}(x, y) = x ^ 2+ (2x - 4y)^2 + (x-5)^2$. Точка минимума $(\frac{5}{2}, \frac{5}{4}, \frac{25}{2})$, \label{item:1}
\item $f_{2}(x, y) = x^2 + y^2 -xy + 2x - 4y + 3$. Точка минимума $(2, 0, -1), $ 
\item Bonus: $f_{3}(x, y) = 100x^2 + y^2$. Точка минимума $(0, 0, 0).$  
\label{item:2}
\end{enumerate}
\endgroup

Каждую из функций мы протестировали на двух методах градиентного спуска, а именно: методе с постоянным шагом и методе одномерного поиска. А также на методе Нельдера-Мида, который заключается в том, что поиск минимума происходит засчет отражений, сжатий и растяжений симплекса. Каждый из методов имеет свои собственные преимущества. Например, градиентные методы являются представителями метода 1 порядка, это значит алгоритм поиска использует производную первого порядка исследуемой функции, в то время как Нельдер-Мид является методом 0 порядка. 
\\\\
\textbf{\textit{Nota bene}} : мы обрезали часть таблицы (последние две колонки, так как они не сильно влияли на результат. Их смысл заключался в критерии остановки метода)\\\\
\title{\textbf{1.  Градиентные методы. Постоянный шаг.}} Реализация алгоритма градиентного спуска на наших функиях работает ожидаемо корректно. Рассмотрим таблицы, в которых отражена работа метода с постоянным шагом. \\
В качестве возможных постоянных шагов мы взяли 0.1, 0.001. Заметно, что выбор шага очень влияет на конечный результат. Так, в 0-3-6 экспериментах для $f_{1}$ минимальное значение так и не достигается, а для бонусной плохо-обусловленной функции 21-24 выявляют тот же результат.\\ С чем это связано? Как известно, следующий шаг градиентного метода вычисляется по формуле $x_{k+1} = x_{k} - \alpha \cdot grad(x_{k}) $. При этом, может случиться такое, что прыжки по вектору градиента устремят точку "вникуда" или просто зациклят. Обычно такое происходит, когда выбранный коэффициент недостаточно мал. Заметно, что, для $f_{1}$ количество итераций наиболее мало, по сравнению с двумя оставшимися. Эта характеристика ни в коем случае не означает, что алгоритм работает плохо для оставшихся функций, так как помимо самого алгоритма важен и выбор начальной точки. В случае с $f_{1}$ нам можно сказать повезло. Из таблицы видно, что метод достаточно точно находит минимальное значение. Различие буквально порядка $10^{-4}$.
\begin{center}

\centering

\includegraphics[width=1\linewidth]{table1.png}

\captionof{figure}{Таблица для $f_{1}$}{}

\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{table2.png}

\captionof{figure}{Таблица для $f_{2}$}{}

\label{fig:mpr}

\end{center}



\begin{center}

\centering

\includegraphics[width=1\linewidth]{table3.png}

\captionof{figure}{Таблица для $f_{3}$}{}

\label{fig:mpr}

\end{center}

Тем не менее такой метод достаточно не конструктивен. Сразу возникает вопрос, а как мы выбрали шаг, который выдаст корректный результат, как мы его подобрали, и что, если для разных функций шаг может варьироваться? Именно поэтому и существует метод градиентного спуска, основанный на одномерном спуске, или как его еще называют "метод наискорейшего спуска".

\title{\textbf{1.  Градиентные методы. Метод наискорейшего спуска.}}.\\ В чем заключается метод? Будем конструктивно искать такое $\alpha$, при котором $f(x_{k} - \alpha \cdot grad(x_{k}))$ принимает минимальное значение по $\alpha$. Заметим, что теперь наша функция является функцией одного аргумента, а также является унимодальной (без док-ва). На помощь приходят методы тернарного спуска и дихотомии (бонус 1). Оба метода сужают область поиска на какую-то фиксированную константу. Такой подход поиска шага более универсален, а засчет правильного подбора мы уменьшим количество итераций алгоритма.

\begin{center}

\centering

\includegraphics[width=1\linewidth]{table4.png}

\captionof{figure}{Таблица для $f_{1}$. Тернарный поиск}{}

\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{table6.png}

\captionof{figure}{Таблица для $f_{2}$. Тернарный поиск}{}

\label{fig:mpr}

\end{center}



\begin{center}

\centering

\includegraphics[width=1\linewidth]{table5.png}

\captionof{figure}{Таблица для $f_{3}$. Тернарный поиск}{}

\label{fig:mpr}

\end{center}



\begin{center}

\centering

\includegraphics[width=1\linewidth]{dichotomy1.png}

\captionof{figure}{Таблица для $f_{1}$. Бонус 1 - дихотомия}{}

\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{dichotomy2.png}

\captionof{figure}{Таблица для $f_{2}$. Бонус 1 - дихотомия}{}

\label{fig:mpr}

\end{center}



\begin{center}

\centering

\includegraphics[width=1\linewidth]{dihotomy3.png}

\captionof{figure}{Таблица для $f_{3}$. Бонус 1 - дихотомия}{}

\label{fig:mpr}

\end{center}


Из таблицы видно, что количество итераций уменьшилось аж в 100 раз. И это неудивительно, потому что каждый раз мы выбираем лучший шаг для данной точки и функции. Конечно, внутри мы крутимся внутри функции поиска, но так или иначе, количество итераций уменьшается, а точноссть значений увеличивается до $10^{-8}$ порядка. Подряд идут фотографии таблиц для тернарного поиска и дихотомии, так как идейно они не сильно отличаются.

\title{\textbf{Нельдер-Мид.}}
\\
Как уже было сказано ранее, Нельдер Мид принципиально другой метод с точки зрения идеи. Основная задача все такая же - поиск минимального значения функции. Но данный метод является методом 0 порядка, а значит нам не нужно брать производную функции. Иногда этот метод довольно удобен если мы хотим исследовать не гладкую функцию. Метод устроен примерно так: берем в n-мерном пространстве симплекс размера n+1, сортируем значения функции в точках симплекса, берем две точки, в которых достигаются максимальные значения функции соответственно, а также точку, где достигается минимальное значение. Считаем центр тяжести n+1 точки и в зависимости от некоторых условий отражаем, сжимаем или растягиваем наш симплекс. Но есть и минусы у этого метода. При выборе "плохих" точек симплекса мы можем убежать дальше допустимой окрестности поиска минимума. Рассмотрим таблицы, относящиеся к Нельдеру-Миду.


\begin{center}

\centering

\includegraphics[width=1\linewidth]{nelder.png}

\captionof{figure}{Общая таблица по Нельдеру-Миду}{}

\label{fig:mpr}

\end{center}

Заметно, что библиотичная реализация этой функции в SciPy достаточно эффективна и точна. Количество итераций чуть больше, чем количество итераций в спуске с одномерным поиском, но значения точны до $10^{-8}$ порядка. Причем видно, что метод достаточно эффективен и для плохо-обусловленной функции.

\title{\textbf{Функция n аргументов. Бонус 2.1}} \\
Мы считаем, что данный бонус ничем не отличается от рассмотренных примеров. Идейно в код нужно добавить точки произвольной размерности и посчитать значения функции с учетом размерности. Код был написан изначально для функции двух аргументов, а принципиально что-то менять для повышения размерности - долго и не представляет научного интереса.



\title{\textbf{Плохо обусловленная функция. Бонус 2.2}} \\
В обоих методах для $f_{3}$ результат в 23 и 26 экспериментах(для постоянного шага) и 21 - 26 экспериментах (для наискорейшего спуска) сильно отличается от желаемого. Дело в том, что данная функция является плохо обусловленной, а именно : при малом изменении нормы аргумента, значения функции меняютcя сильно. Это затрудняет выдачу точных значений для градиентных методов. В таком случае лучше использовать Нельдера-Мида.

\title{\textbf{Зашумленная функция и Мультимодальная функция. Бонус 2.3}}\\
В чем смысл зашумленной функции? Смысл в том, что "поставщик" значений может прислать значения функции с немного некорректными значениями. В общем виде это выглядит так $f \rightarrow f + random(0, EPS)$.\\ При этом поставщик значений ожидает, что минимальное значение при зашумлении не сильно будет отличаться от минимального значения оригинальной функции. Так вышло, что зашумление достаточно сильно влияет на поиск минимального значения. Это немудрено, так как сама функция становится недифференцируемой и неявной. Рассмотрим таблицы для постоянного шага и для изменяемого.

\begin{center}

\centering

\includegraphics[width=1\linewidth]{noisyConstant.jpg}

\captionof{figure}{Зашумленная функция. Постоянный шаг. Бонус 2.3}{}

\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{noisyChanged.jpg}

\captionof{figure}{Зашумленная функция. Изменяемый  шаг. Бонус 2.3}


\label{fig:mpr}

\end{center}

Видно, что количество итераций может достигать больших значений. Да и вообще, нельзя рассчитывать на хоть сколько-то корректные значения с неявной и недифференцируемой функцией. \\\\

Рассмотрим мультимодальную функцию. В качестве примера мы взяли функцию Экли. В чем сложность работы с мультимодальными функциями? Мультимодальная функция — это функция с более чем одной “модой” или оптимумом (например долиной на графике). А значит, градиентный метод может забрести не в ту впадину и мы получим не тот ответ, который ожидаем. Мы ожидаем в качестве минимального значения выбранной функции 0. Рассмотрим таблицы.

\begin{center}

\centering

\includegraphics[width=1\linewidth]{eckleylearning.jpg}

\captionof{figure}{Функция Экли. Постоянный шаг. Бонус 2.3}


\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{eckleychanging.jpg}

\captionof{figure}{Функция Экли. Тернарный поиск. Бонус 2.3}


\label{fig:mpr}

\end{center}


\begin{center}

\centering

\includegraphics[width=1\linewidth]{eckleyDih.jpg}

\captionof{figure}{Функция Экли. Дихотомия. Бонус 2.3}


\label{fig:mpr}

\end{center}
Видно, что метод тернарного поиска для этой мультимодальной функции преуспел. Из этого следует, что для мультимодальных методов градиентный метод не подходит в силу нескольких оптимумов.\\

\title{\textbf{Картинки:}} \\
\begin{center}

\centering

\includegraphics[width=1\linewidth]{gradient.png}

\captionof{figure}{Блуждания градиента}


\label{fig:mpr}

\end{center}

\begin{center}

\centering

\includegraphics[width=1\linewidth]{level_lines.png}

\captionof{figure}{Функция и линии уровня}


\label{fig:mpr}

\end{center}

\title{\textbf{Итог:}} \\
В ходе лабораторной работы мы узнали много удивительных фактов, а также поработали с разными типами функций, численно нашли значение минимумов и оценили отклонения.


\end{document}
